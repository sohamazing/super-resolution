# Photorealistic 4x Image Super-Resolution with Real-ESRGAN

This project is a from-scratch implementation of a state-of-the-art, photorealistic 4x image super-resolution model in PyTorch. The architecture and training strategy are heavily inspired by the famous Real-ESRGAN paper, designed to produce visually pleasing results that excel in perceptual quality.

The entire workflow, from data preparation to a professional training pipeline with experiment tracking, is included.

## Features

* **Deep Generator:** A powerful generator network using a deep stack of **Residual-in-Residual Dense Blocks (RRDBs)** for complex feature extraction.
* **VGG-Style Discriminator:** A robust discriminator with **Spectral Normalization** to ensure stable and effective adversarial training.
* **Advanced Loss Functions:** Combines L1 pixel loss, adversarial loss, and **VGG Perceptual Loss** to optimize for realistic textures and details.
* **Professional Training Pipeline:** Features a two-phase training process (pre-training and GAN training), checkpointing, and full resume capabilities.
* **Experiment Tracking:** Integrated with **Weights & Biases (`wandb`)** to provide real-time monitoring of losses, learning rates, and visual results.
* **Modular Codebase:** The project is organized into clear, distinct scripts for data preparation (`create_patches.py`), the models (`generator.py`, `discriminator.py`), and training (`train.py`).

## Project Structure

```
super-res/
├── data/                    # (Generated by create_patches.py)
│   ├── train/
│   │   ├── HR/
│   │   └── LR/
│   ├── val/
│   │   ├── HR/
│   │   └── LR/
│   └── test/
│       ├── HR/
│       └── LR/
├── create_patches.py        # Script to process source images into a training dataset.
├── generator.py             # Contains the GeneratorRRDB model architecture.
├── discriminator.py         # Contains the Discriminator model architecture.
├── train.py                 # The main script for pre-training and GAN training.
└── README.md                # This file.
```

## Workflow & How to Use

This project follows a three-step workflow: Setup, Data Preparation, and Training.

### 1. Setup

First, create the conda environment and install all required dependencies.

```bash
# Create and activate the environment
conda create --name super-res python=3.10 -y
conda activate super-res

# Install all necessary packages
pip install torch torchvision
pip install opencv-python numpy tqdm wandb
pip install imageio imageio-ffmpeg rawpy pillow-heif
```

### 2. Data Acquisition & Setup

This project is configured to use the **DIV2K** and **Flickr2K** datasets.

1.  **Download the datasets:**
    * **DIV2K:** Download `DIV2K_train_HR` (800 images) and `DIV2K_valid_HR` (100 images) from the [official homepage](https://data.vision.ee.ethz.ch/cvl/DIV2K/).
    * **Flickr2K:** Download the `Flickr2K_HR` images (2650 images) from the [author's homepage](http://cv.snu.ac.kr/research/EDSR/Flickr2K.tar).
2.  **Organize the folders:** Place the downloaded and unzipped image folders in a convenient location, for example `~/Desktop/Photos/`.

### 3. Data Preparation

The `create_patches.py` script processes the high-resolution source images into a structured dataset of low-resolution and high-resolution patches for training.

1.  **Configure `create_patches.py`:** Open the script and edit the `TRAIN_VAL_SOURCES` and `TEST_SOURCES` lists to point to the exact locations of your downloaded datasets.

    ```python
    # In create_patches.py
    TRAIN_VAL_SOURCES: list = field(default_factory=lambda: [
        Path.home() / "Desktop" / "Photos" / "Flickr2K" / "Flickr2K_HR",
        Path.home() / "Desktop" / "Photos" / "Div2K" / "DIV2K_train_HR",
    ])
    TEST_SOURCES: list = field(default_factory=lambda: [
        Path.home() / "Desktop" / "Photos" / "Div2K" / "DIV2K_valid_HR"
    ])
    ```
2.  **Run the script:** From your terminal, run the script. This will be CPU-intensive and will generate a `data` folder in your project directory.
    ```bash
    python3 create_patches.py
    ```

### 4. Training the Model

The `train.py` script handles both the initial generator pre-training and the main adversarial (GAN) training.

1.  **Set Up `wandb`:** Log in to your Weights & Biases account.
    ```bash
    wandb login
    ```
2.  **Phase 1: Pre-training (Optional but Recommended):** If a `generator_pretrained.pth` file does not exist, the script will automatically run a "warmup" phase where the generator is trained with a simple pixel loss. This stabilizes the main training.
    ```bash
    # This command will automatically run pre-training if needed
    python3 train.py
    ```
3.  **Phase 2: Main GAN Training:** After pre-training, the script automatically begins the main adversarial training loop.
4.  **Monitoring:** Track your progress, view loss curves, and see visual results by clicking the `wandb.ai` link that appears in your terminal when training starts.
5.  **Resuming Training:** The script saves checkpoints every 10 epochs. If your training is interrupted, you can resume it perfectly by using the `--resume_epoch` flag. For example, to resume from the checkpoint saved at epoch 20:
    ```bash
    python3 train.py --resume_epoch 20
    ```

---

### Using Personal Photo Libraries

This project also contains workflows for using your own photos.
* **Apple Photos:** An included AppleScript can be used to safely batch-export original-quality photos.
* **Google Drive / Google Photos:** The `gather_photos.py` script can be configured to download entire folders from Google Drive (ideal for using with Google Takeout exports from Google Photos).

To use these, place the exported/downloaded photos into a folder and add its path to the `TRAIN_VAL_SOURCES` list in `create_patches.py`.

---

### Model Architecture

* **Generator (`generator.py`):** A deep network of 23 Residual-in-Residual Dense Blocks (RRDB) with learnable upsampling via `PixelShuffle`. This architecture allows for the extraction of incredibly detailed and hierarchical features.
* **Discriminator (`discriminator.py`):** A deep, VGG-style network that acts as a patch-based classifier to determine if an image is real or generated. Spectral Normalization is used on all convolutional layers to enforce the Lipschitz constraint and stabilize GAN training dynamics.